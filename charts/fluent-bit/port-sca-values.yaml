USER-SUPPLIED VALUES:
affinity: {}
config:
  customParsers: |
    [PARSER]
        Name         custom_cri
        Format       regex
        Regex ^(?<timestamp>[^ ]+) (?<stream>stdout|stderr) (?<logtag>[^ ]*) (?<log>.*)$
        Time_Key    timestamp
        Time_Format %Y-%m-%dT%H:%M:%S.%L%z

    [PARSER]
        Name grepallline
        Format regex
        Regex ^(?<Message>.*)$

    [PARSER]
        Name   apache
        Format regex
        Regex  ^(?<host>[^ ]*) [^ ]* (?<user>[^ ]*) \[(?<time>[^\]]*)\] "(?<method>\S+)(?: +(?<path>[^\"]*?)(?: +\S*)?)?" (?<code>[^ ]*) (?<size>[^ ]*)(?: "(?<referer>[^\"]*)" "(?<agent>[^\"]*)")?$
        Time_Key time
        Time_Format %d/%b/%Y:%H:%M:%S %z

    [PARSER]
        Name   apache2
        Format regex
        Regex  ^(?<host>[^ ]*) [^ ]* (?<user>[^ ]*) \[(?<time>[^\]]*)\] "(?<method>\S+)(?: +(?<path>[^ ]*) +\S*)?" (?<code>[^ ]*) (?<size>[^ ]*)(?: "(?<referer>[^\"]*)" "(?<agent>[^\"]*)")?$
        Time_Key time
        Time_Format %d/%b/%Y:%H:%M:%S %z

    [PARSER]
        Name   apache_error
        Format regex
        Regex  ^\[[^ ]* (?<time>[^\]]*)\] \[(?<level>[^\]]*)\](?: \[pid (?<pid>[^\]]*)\])?( \[client (?<client>[^\]]*)\])? (?<message>.*)$

    [PARSER]
        Name   nginx
        Format regex
        Regex ^(?<remote>[^ ]*) (?<host>[^ ]*) (?<user>[^ ]*) \[(?<time>[^\]]*)\] "(?<method>\S+)(?: +(?<path>[^\"]*?)(?: +\S*)?)?" (?<code>[^ ]*) (?<size>[^ ]*)(?: "(?<referer>[^\"]*)" "(?<agent>[^\"]*)")?$
        Time_Key time
        Time_Format %d/%b/%Y:%H:%M:%S %z

    [PARSER]
        Name   json
        Format json
        Time_Key time
        # [2023/01/24 13:26:58] [error] [parser] cannot parse '2023-01-24T13:26:58Z'
        # [2023/01/24 13:26:58] [ warn] [parser:json] invalid time format %d/%b/%Y:%H:%M:%S %z for '2023-01-24T13:26:58Z'
        # Time_Format %d/%b/%Y:%H:%M:%S %z
        Time_Format %Y-%m-%dT%H:%M:%S %z

    [PARSER]
        Name        docker
        Format      json
        Time_Key    time
        Time_Format %Y-%m-%dT%H:%M:%S.%L
        Time_Keep   On

    [PARSER]
        Name        syslog
        Format      regex
        Regex       ^\<(?<pri>[0-9]+)\>(?<time>[^ ]* {1,2}[^ ]* [^ ]*) (?<host>[^ ]*) (?<ident>[a-zA-Z0-9_\/\.\-]*)(?:\[(?<pid>[0-9]+)\])?(?:[^\:]*\:)? *(?<message>.*)$
        Time_Key    time
        Time_Format %b %d %H:%M:%S

    [STREAM_TASK]
        Name find_metrics
        Exec CREATE STREAM metrics AS SELECT * from TAG:'*scan-logs*' WHERE @record.contains(is_metric);

    # if there is a log record the parsing failed
    [STREAM_TASK]
        Name find_none_jsons
        Exec CREATE STREAM nojson AS SELECT * from TAG:'*scan-logs*' WHERE @record.contains(log);

    # If Level exists it is a log not a metric or a nojson
    [STREAM_TASK]
        Name fix_nojson
        Exec CREATE STREAM okfixed AS SELECT * from STREAM:nojson WHERE @record.contains(Level);

    [STREAM_TASK]
        Name find_oks
        Exec CREATE STREAM oksource AS SELECT * from TAG:'*scan-logs*' WHERE @record.contains(Level);
  filters: |
    [FILTER]
        Name             parser
        Match            scan-logs.*
        Key_Name         log
        Parser           json
        Reserve_Data     True
    
    [FILTER]
        Name             kubernetes
        Match            scan-logs.*
        Kube_Tag_Prefix  scan-logs.var.log.containers.
        Kube_URL         https://kubernetes.default.svc:443
        Kube_CA_File     /var/run/secrets/kubernetes.io/serviceaccount/ca.crt
        Kube_Token_File  /var/run/secrets/kubernetes.io/serviceaccount/token
        Labels           Off
        Annotations      Off
        Buffer_Size      50MB
        K8S-Logging.Parser  Off
        K8S-Logging.Exclude On
    
    [FILTER]
        Name             modify
        Match            scan-logs.*
        Rename           level Level
        Rename           msg Message
    
    [FILTER]
        Name             record_modifier
        Match            *
        Record           cluster ${ENVIRONMENT}
    
    [FILTER]
        Name             lua
        Match            nojson
        script           /fluent-bit/scripts/lua_functions.lua
        call             append_date
    
    [FILTER]
        Name             record_modifier
        Match            nojson
        Record           Level info
    
    [FILTER]
        Name             parser
        Match            okfixed
        Key_Name         log
        Parser           grepallline
        Reserve_Data     True
    
    [FILTER]
        Name             lua
        Match            ok*
        script           /fluent-bit/scripts/lua_functions.lua
        call             append_event_dataset
    
    [FILTER]
        Name             rewrite_tag
        Match            metrics*
        Rule             $enable_bucket ^(true|True)$  metrics-${ENVIRONMENT} true
        Emitter_Name     re_emitted
    
    [FILTER]
        Name             modify
        Match            *
        Rename           container_id container.id
        Rename           container_name container.name
    
    [FILTER]
        Name             modify
        Match            metrics*
        Remove           enable_bucket
    
    [FILTER]
        Name             rewrite_tag
        Match            metrics*
        Rule             $dimensions['trace_id'] ^.*$  metrics-pipeline false
        Emitter_Name     trace_emmiter
  inputs: |
    [INPUT]
        Name    tail
        Tag     scan-logs.*
        Path    /var/log/containers/*-sca-*.log,/var/log/containers/*_sca-*.log
        # Exclude_Path /var/log/containers/*-sca-*.log,/var/log/containers/*_sca-*.log,/var/log/containers/fluent-bit*.log,/var/log/containers/metricbeat*.log,/var/log/containers/aws-ecr-credential*.log,/var/log/containers/*platform-minio*.log
        Exclude_Path /var/log/containers/fluent-bit*.log,/var/log/containers/metricbeat*.log,/var/log/containers/*platform-minio*.log
        Read_from_head true
        Parser  custom_cri
        Skip_Long_Lines   On
        Refresh_Interval  10
        Buffer_Chunk_Size 1MB
        Buffer_Max_Size   1MB
        # Mem_Buf_Limit     1536MB
        # resolving OOMKilled of fluent-bit-sca pods periodic issue for Mem_Buf_Limit 256M
        Mem_Buf_Limit     512MB
  outputs: |
    [OUTPUT]
        Name               es
        Match              ok*
        Host               ${ELASTICSEARCH_HOST}
        Port               ${ELASTICSEARCH_PORT}
        HTTP_User          ${ELASTICSEARCH_USERNAME}
        HTTP_Passwd        ${ELASTICSEARCH_PASSWORD}
        Index              ${INDEX_NAME}-logs
        TLS                On
        tls.verify         Off
        Include_Tag_Key    true
        Replace_Dots       On
        Trace_Error        Off
        Trace_Output       Off
        Buffer_Size        5MB
        net.connect_timeout 30
        net.keepalive      on
        net.keepalive_idle_timeout 10
        workers            5
        Suppress_Type_Name On
    
    [OUTPUT]
        Name               es
        Match              metrics
        Host               ${ELASTICSEARCH_HOST}
        Port               ${ELASTICSEARCH_PORT}
        Index              ${INDEX_NAME}-metrics
        HTTP_User          ${ELASTICSEARCH_USERNAME}
        HTTP_Passwd        ${ELASTICSEARCH_PASSWORD}
        TLS                On
        tls.verify         Off
        Include_Tag_Key    true
        Time_Key           es_time
        Suppress_Type_Name On
  service: |
    [SERVICE]
        Flush 60
        # Flush 10
        Log_Level {{ .Values.logLevel }}
        Daemon        off
        # Parsers_File parsers.conf
        Parsers_File custom_parsers.conf
        # Streams_File stream_processor.conf
        Streams_File    custom_parsers.conf
        HTTP_Server on
        HTTP_Listen 0.0.0.0
        HTTP_Port {{ .Values.service.port }}
        Health_Check On
dashboards:
  annotations: {}
  enabled: true
  labelKey: grafana_dashboard
dnsConfig: {}
env:
- name: ELASTICSEARCH_USERNAME
  value: fluentbit
- name: ELASTICSEARCH_PASSWORD
  value: ""
- name: ELASTICSEARCH_HOST
  value: bd311f7d4c9140899da07d99f07b0011.vpce.eu-west-1.aws.elastic-cloud.com
- name: ELASTICSEARCH_PORT
  value: "9243"
- name: ELASTIC_CLOUD_ID
  value: ""
- name: ELASTIC_CLOUD_AUTH
  value: ""
- name: ENVIRONMENT
  value: cx-components-ci
- name: INDEX_NAME
  value: cx-components-ci-sca
- name: INDEX_NAME_PREFIX
  value: cx
envFrom: []
existingConfigMap: ""
extraPorts: []
extraVolumeMounts: []
extraVolumes: []
fullnameOverride: ""
image:
  pullPolicy: Always
  repository: fluent/fluent-bit
  tag: 2.0.8
imagePullSecrets: []
kind: DaemonSet
livenessProbe:
  httpGet:
    path: /
    port: http
logLevel: warn
luaScripts:
  lua_functions.lua: |-
    function append_date(tag, timestamp, record)
        new_record = record
        new_record["@timestamp"] = os.date("%Y-%m-%dT%X.0000000Z")
        return 1, timestamp, new_record
    end

    function append_event_dataset(tag, timestamp, record)
        new_record = record
        if record["serviceName"] == nil then
          new_record["event.dataset"] = 'NoServiceProvided' .. "." .. record["Level"]
        else
          new_record["event.dataset"] = record["serviceName"] .. "." .. record["Level"]
        end
        return 1, timestamp, new_record
    end
nameOverride: ""
networkPolicy:
  enabled: false
nodeSelector: {}
podAnnotations:
  fluentbit.io/exclude: "true"
podLabels: {}
podSecurityContext: {}
podSecurityPolicy:
  create: false
priorityClassName: ""
prometheusRule:
  enabled: false
rbac:
  create: true
readinessProbe:
  httpGet:
    path: /api/v1/health
    port: http
resources:
  limits:
    # cpu: 500m
    memory: 1536Mi
  requests:
    cpu: 100m
    memory: 512Mi
securityContext: {}
service:
  annotations: {}
  labels: {}
  port: 2020
  type: ClusterIP
serviceAccount:
  annotations: {}
  create: true
  name: null
serviceMonitor:
  enabled: false
testFramework:
  image:
    pullPolicy: Always
    repository: busybox
    tag: latest
tolerations:
        - key: "environment"
          operator: "Equal"
          value: "prod"
          effect: "NoSchedule"
        - key: "service"
          operator: "Equal"
          value: "sca-source-resolver"
          effect: "NoSchedule"
updateStrategy:
  rollingUpdate:
    maxUnavailable: 1
  type: RollingUpdate
