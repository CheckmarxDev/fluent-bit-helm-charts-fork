USER-SUPPLIED VALUES:
affinity: {}
config:
  customParsers: |
    [PARSER]
        Name         custom_cri
        Format       regex
        Regex ^(?<timestamp>[^ ]+) (?<stream>stdout|stderr) (?<logtag>[^ ]*) (?<log>.*)$
        Time_Key    timestamp
        Time_Format %Y-%m-%dT%H:%M:%S.%L%z

    [PARSER]
        Name grepallline
        Format regex
        Regex ^(?<Message>.*)$

    [PARSER]
        Name   apache
        Format regex
        Regex  ^(?<host>[^ ]*) [^ ]* (?<user>[^ ]*) \[(?<time>[^\]]*)\] "(?<method>\S+)(?: +(?<path>[^\"]*?)(?: +\S*)?)?" (?<code>[^ ]*) (?<size>[^ ]*)(?: "(?<referer>[^\"]*)" "(?<agent>[^\"]*)")?$
        Time_Key time
        Time_Format %d/%b/%Y:%H:%M:%S %z

    [PARSER]
        Name   apache2
        Format regex
        Regex  ^(?<host>[^ ]*) [^ ]* (?<user>[^ ]*) \[(?<time>[^\]]*)\] "(?<method>\S+)(?: +(?<path>[^ ]*) +\S*)?" (?<code>[^ ]*) (?<size>[^ ]*)(?: "(?<referer>[^\"]*)" "(?<agent>[^\"]*)")?$
        Time_Key time
        Time_Format %d/%b/%Y:%H:%M:%S %z

    [PARSER]
        Name   apache_error
        Format regex
        Regex  ^\[[^ ]* (?<time>[^\]]*)\] \[(?<level>[^\]]*)\](?: \[pid (?<pid>[^\]]*)\])?( \[client (?<client>[^\]]*)\])? (?<message>.*)$

    [PARSER]
        Name   nginx
        Format regex
        Regex ^(?<remote>[^ ]*) (?<host>[^ ]*) (?<user>[^ ]*) \[(?<time>[^\]]*)\] "(?<method>\S+)(?: +(?<path>[^\"]*?)(?: +\S*)?)?" (?<code>[^ ]*) (?<size>[^ ]*)(?: "(?<referer>[^\"]*)" "(?<agent>[^\"]*)")?$
        Time_Key time
        Time_Format %d/%b/%Y:%H:%M:%S %z

    [PARSER]
        Name   json
        Format json
        Time_Key time
        # [2023/01/24 13:26:58] [error] [parser] cannot parse '2023-01-24T13:26:58Z'
        # [2023/01/24 13:26:58] [ warn] [parser:json] invalid time format %d/%b/%Y:%H:%M:%S %z for '2023-01-24T13:26:58Z'
        # Time_Format %d/%b/%Y:%H:%M:%S %z
        Time_Format %Y-%m-%dT%H:%M:%S %z

    [PARSER]
        Name        docker
        Format      json
        Time_Key    time
        Time_Format %Y-%m-%dT%H:%M:%S.%L
        Time_Keep   On

    [PARSER]
        Name        syslog
        Format      regex
        Regex       ^\<(?<pri>[0-9]+)\>(?<time>[^ ]* {1,2}[^ ]* [^ ]*) (?<host>[^ ]*) (?<ident>[a-zA-Z0-9_\/\.\-]*)(?:\[(?<pid>[0-9]+)\])?(?:[^\:]*\:)? *(?<message>.*)$
        Time_Key    time
        Time_Format %b %d %H:%M:%S

    [STREAM_TASK]
        Name find_metrics
        Exec CREATE STREAM metrics AS SELECT * from TAG:'*scan-logs*' WHERE @record.contains(is_metric);

    # if there is a log record the parsing failed
    [STREAM_TASK]
        Name find_none_jsons
        Exec CREATE STREAM nojson AS SELECT * from TAG:'*scan-logs*' WHERE @record.contains(log);

    # If Level exists it is a log not a metric or a nojson
    [STREAM_TASK]
        Name fix_nojson
        Exec CREATE STREAM okfixed AS SELECT * from STREAM:nojson WHERE @record.contains(Level);

    [STREAM_TASK]
        Name find_oks
        Exec CREATE STREAM oksource AS SELECT * from TAG:'*scan-logs*' WHERE @record.contains(Level);
  filters: "[FILTER]\n    Name parser\n    Match scan-logs.*\n    # Match scan-logs*\n
    \   Key_Name log\n    Parser json\n    # Parser custom_cri\n    Reserve_Data True\n\n[FILTER]\n
    \   Name             kubernetes\n    # Match            kube.*\n    Match            scan-logs.*\n
    \   Kube_Tag_Prefix  scan-logs.var.log.containers.\n    Kube_URL         https://kubernetes.default.svc:443\n
    \   Kube_CA_File     /var/run/secrets/kubernetes.io/serviceaccount/ca.crt\n    Kube_Token_File
    \ /var/run/secrets/kubernetes.io/serviceaccount/token\n    # Merge_Log        On\n
    \   # Merge_Log_Key    log_processed\n    # Merge_Log_Trim   On\n    Labels           Off\n
    \   Annotations      Off\n    Buffer_Size      50MB\n    K8S-Logging.Parser  Off\n
    \   K8S-Logging.Exclude On\n\n[FILTER]\n    Name aws\n    Match scan-logs.*\n
    \   imds_version v2\n    az true\n    ec2_instance_id true\n    ec2_instance_type
    true\n    private_ip true\n    ami_id true\n    account_id true\n    hostname
    true\n    vpc_id true\n\n# Added by Vladimir in order to \n[FILTER]\n    Name
    modify\n    Match scan-logs.*\n    # Add Service1 SOMEVALUE\n    # Add Comment
    \"Manual_level_Change\"\n    # Rename Mem.free MEMFREE\n    Rename level Level\n
    \   Rename msg Message\n\n\n[FILTER]\n    Name      record_modifier\n    match
    \    *\n    record    cluster ${ENVIRONMENT}\n\n# [FILTER]\n#     Name parser\n#
    \    Match *\n#     Key_Name log\n#     Parser json\n#     # Parser custom_cri\n#
    \    Reserve_Data True\n\n[FILTER]\n    Name lua\n    Match nojson\n    script
    /fluent-bit/scripts/lua_functions.lua\n    call append_date\n\n[FILTER]\n    Name
    record_modifier\n    Match nojson\n    Record Level info\n\n# commented with Tal
    Roth\n# [FILTER]\n#     Name record_modifier\n#     Match metrics\n#     Record
    service_name ${SERVICENAME}\n#     Record environment ${ENVIRONMENT}\n#     Record
    enable_bucket ${ENABLE_METRICS_BUCKET}\n\n[FILTER]\n    Name parser\n    Match
    okfixed\n    Key_Name log\n    Parser grepallline\n    Reserve_Data True\n\n#
    commented with Tal Roth\n# [FILTER]\n#     Name record_modifier\n#     Match ok*\n#
    \    Record environment ${ENVIRONMENT}\n#     Record serviceName ${SERVICENAME}\n\n[FILTER]\n
    \   Name lua\n    Match ok*\n    script /fluent-bit/scripts/lua_functions.lua\n
    \   call append_event_dataset\n\n[FILTER]\n    Name          rewrite_tag\n    Match
    \        metrics*\n    Rule          $enable_bucket ^(true|True)$  metrics-${ENVIRONMENT}
    true\n    Emitter_Name  re_emitted\n    \n# rename container fields to kibana
    conventions \n[FILTER]\n    Name modify\n    Match *\n    Rename container_id
    container.id\n    Rename container_name container.name\n\n[FILTER]\n    Name modify\n
    \   Match metrics*\n    Remove enable_bucket\n\n# rewrite tag for metrics with
    enrichment\n[FILTER]\n    Name          rewrite_tag\n    Match         metrics*\n
    \   Rule          $dimensions['trace_id'] ^.*$  metrics-pipeline false\n    Emitter_Name
    \ trace_emmiter\n"
  inputs: |
    [INPUT]
        Name    tail
        Tag     scan-logs.*
        Path    /var/log/containers/*-sca-*.log,/var/log/containers/*_sca-*.log
        # Exclude_Path /var/log/containers/*-sca-*.log,/var/log/containers/*_sca-*.log,/var/log/containers/fluent-bit*.log,/var/log/containers/metricbeat*.log,/var/log/containers/aws-ecr-credential*.log,/var/log/containers/*platform-minio*.log
        Exclude_Path /var/log/containers/fluent-bit*.log,/var/log/containers/metricbeat*.log,/var/log/containers/*platform-minio*.log
        Read_from_head true
        Parser  custom_cri
        Skip_Long_Lines   On
        Refresh_Interval  10
        Buffer_Chunk_Size 1MB
        Buffer_Max_Size   1MB
        # Mem_Buf_Limit     1536MB
        # resolving OOMKilled of fluent-bit-sca pods periodic issue for Mem_Buf_Limit 256M
        Mem_Buf_Limit     512MB
  outputs: "# Logs \n[OUTPUT]\n    Name es\n    # Match scan-logs.*\n    Match ok*\n
    \   Host ${ELASTICSEARCH_HOST}\n    Port ${ELASTICSEARCH_PORT}\n    HTTP_User
    ${ELASTICSEARCH_USERNAME}\n    HTTP_Passwd ${ELASTICSEARCH_PASSWORD}\n    Index
    ${INDEX_NAME}-logs\n    TLS on\n    Include_Tag_Key true\n    Replace_Dots On\n
    \   Trace_Error Off\n    Trace_Output Off\n    # Generate_ID On\n    Buffer_Size
    100MB\n    net.connect_timeout 30\n    net.keepalive on\n    net.keepalive_idle_timeout
    10\n    workers         5\n    Suppress_Type_Name  On\n\n[OUTPUT]\n    Name es\n
    \   Match metrics\n    Host            ${ELASTICSEARCH_HOST}\n    Port            ${ELASTICSEARCH_PORT}\n
    \   Index           ${INDEX_NAME}-metrics\n    HTTP_User       ${ELASTICSEARCH_USERNAME}\n
    \   HTTP_Passwd     ${ELASTICSEARCH_PASSWORD}\n    TLS on\n    Include_Tag_Key
    true\n    Time_Key es_time\n    Suppress_Type_Name  On\n"
  service: |
    [SERVICE]
        Flush 60
        # Flush 10
        Log_Level {{ .Values.logLevel }}
        Daemon        off
        # Parsers_File parsers.conf
        Parsers_File custom_parsers.conf
        # Streams_File stream_processor.conf
        Streams_File    custom_parsers.conf
        HTTP_Server on
        HTTP_Listen 0.0.0.0
        HTTP_Port {{ .Values.service.port }}
        Health_Check On
dashboards:
  annotations: {}
  enabled: true
  labelKey: grafana_dashboard
dnsConfig: {}
env:
- name: ELASTICSEARCH_USERNAME
  value: fluentbit
- name: ELASTICSEARCH_PASSWORD
  value: ""
- name: ELASTICSEARCH_HOST
  value: bd311f7d4c9140899da07d99f07b0011.vpce.eu-west-1.aws.elastic-cloud.com
- name: ELASTICSEARCH_PORT
  value: "9243"
- name: ELASTIC_CLOUD_ID
  value: ""
- name: ELASTIC_CLOUD_AUTH
  value: ""
- name: ENVIRONMENT
  value: cx-components-ci
- name: INDEX_NAME
  value: cx-components-ci-sca
- name: INDEX_NAME_PREFIX
  value: cx
envFrom: []
existingConfigMap: ""
extraPorts: []
extraVolumeMounts: []
extraVolumes: []
fullnameOverride: ""
image:
  pullPolicy: Always
  repository: fluent/fluent-bit
  tag: 2.0.8
imagePullSecrets: []
kind: DaemonSet
livenessProbe:
  httpGet:
    path: /
    port: http
logLevel: warn
luaScripts:
  lua_functions.lua: |-
    function append_date(tag, timestamp, record)
        new_record = record
        new_record["@timestamp"] = os.date("%Y-%m-%dT%X.0000000Z")
        return 1, timestamp, new_record
    end

    function append_event_dataset(tag, timestamp, record)
        new_record = record
        if record["serviceName"] == nil then
          new_record["event.dataset"] = 'NoServiceProvided' .. "." .. record["Level"]
        else
          new_record["event.dataset"] = record["serviceName"] .. "." .. record["Level"]
        end
        return 1, timestamp, new_record
    end
nameOverride: ""
networkPolicy:
  enabled: false
nodeSelector: {}
podAnnotations:
  fluentbit.io/exclude: "true"
podLabels: {}
podSecurityContext: {}
podSecurityPolicy:
  create: false
priorityClassName: ""
prometheusRule:
  enabled: false
rbac:
  create: true
readinessProbe:
  httpGet:
    path: /api/v1/health
    port: http
resources:
  limits:
    memory: 256Mi
  requests:
    cpu: 100m
    memory: 128Mi
securityContext: {}
service:
  annotations: {}
  labels: {}
  port: 2020
  type: ClusterIP
serviceAccount:
  annotations: {}
  create: true
  name: null
serviceMonitor:
  enabled: false
testFramework:
  image:
    pullPolicy: Always
    repository: busybox
    tag: latest
tolerations: null
updateStrategy:
  rollingUpdate:
    maxUnavailable: 1
  type: RollingUpdate
